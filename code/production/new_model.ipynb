{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, PReLU, Input, Reshape, Layer, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "def build_encoder(input_shape, latent_dim):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    # Convolutional Layers\n",
    "    x = Conv1D(filters=128, kernel_size=5, padding='valid', name='conv1')(input_layer)\n",
    "    x = PReLU(name='prelu1')(x)\n",
    "    x = MaxPooling1D(pool_size=2, name='maxpool1')(x)\n",
    "\n",
    "    x = Conv1D(filters=256, kernel_size=11, padding='valid', name='conv2')(x)\n",
    "    x = PReLU(name='prelu2')(x)\n",
    "    x = MaxPooling1D(pool_size=2, name='maxpool2')(x)\n",
    "\n",
    "    x = Conv1D(filters=512, kernel_size=21, padding='valid', name='conv3')(x)\n",
    "    x = PReLU(name='prelu3')(x)\n",
    "    x = MaxPooling1D(pool_size=2, name='maxpool3')(x)\n",
    "\n",
    "    # Flatten the output from Conv layers\n",
    "    x = Flatten(name='flatten')(x)\n",
    "\n",
    "    # Fully Connected Layers\n",
    "    x = Dense(256, name='dense1')(x)\n",
    "    x = PReLU(name='prelu4')(x)\n",
    "    x = Dense(128, name='dense2')(x)\n",
    "    x = PReLU(name='prelu5')(x)\n",
    "    x = Dense(64, name='dense3')(x)\n",
    "    x = PReLU(name='prelu6')(x)\n",
    "\n",
    "    # Latent Space\n",
    "    latent_space = Dense(latent_dim, name='latent_space')(x)\n",
    "\n",
    "    return Model(input_layer, latent_space, name='encoder')\n",
    "\n",
    "# This version compiles but has run time issues. \n",
    "'''\n",
    "def build_decoder(latent_dim, output_dim, rest_range, observed_range, observed_resolution, upsample_factor):\n",
    "    # Inputs: latent vector and scalar z\n",
    "    latent_input = Input(shape=(latent_dim,), name='latent_input')\n",
    "    z_input = Input(shape=(1,), name='z')  # Scalar input for z\n",
    "    \n",
    "    # Step 1: Fully Connected Layers to generate rest frame representation\n",
    "    x = Dense(64)(latent_input)\n",
    "    x = PReLU()(x)    \n",
    "    x = Dense(256)(x)\n",
    "    x = PReLU()(x)\n",
    "    x = Dense(1024)(x)\n",
    "    x = PReLU()(x)\n",
    "\n",
    "    # Generate rest frame grid\n",
    "    min_rest_x, max_rest_x = rest_range\n",
    "    rest_length = int((max_rest_x - min_rest_x) / observed_resolution * upsample_factor)\n",
    "    rest_x = tf.linspace(min_rest_x, max_rest_x, rest_length)\n",
    "\n",
    "    # Upsample Layer \n",
    "    x = Dense(rest_length)(x)\n",
    "    x = PReLU()(x)\n",
    "\n",
    "    # Define a Lambda layer to handle TensorFlow operations on Keras tensors\n",
    "    def slice_and_downsample(inputs):\n",
    "        x, z_input = inputs\n",
    "\n",
    "        # Compute Boundary Indexes based on rest_x, observed_range, and z_input\n",
    "        min_rest_obs = observed_range[0] / (1 + z_input)\n",
    "        max_rest_obs = observed_range[1] / (1 + z_input)\n",
    "\n",
    "        # Find start and stop indices using TensorFlow operations\n",
    "        start_index = tf.argmin(tf.abs(rest_x - min_rest_obs))\n",
    "        stop_index = tf.argmin(tf.abs(rest_x - max_rest_obs))\n",
    "\n",
    "        # Slice and downsample the tensor\n",
    "        sliced_x = x[:, start_index:stop_index]\n",
    "\n",
    "        # Ensure downsampling results in `output_dim` elements\n",
    "        downsample_factor = tf.cast(tf.shape(sliced_x)[1] / output_dim, tf.int32)\n",
    "        output = sliced_x[:, ::downsample_factor]\n",
    "\n",
    "        return output\n",
    "\n",
    "    # Define the output shape manually\n",
    "    def compute_output_shape(input_shapes):\n",
    "        latent_shape, z_shape = input_shapes\n",
    "        return (latent_shape[0], output_dim)\n",
    "\n",
    "    # Use Lambda layer to apply the slicing and downsampling\n",
    "    output = Lambda(slice_and_downsample, output_shape=compute_output_shape)([x, z_input])\n",
    "\n",
    "    # Reshape the final output to the desired dimensions\n",
    "    output = Reshape((output_dim, 1))(output)\n",
    "\n",
    "    return Model([latent_input, z_input], output, name='decoder')\n",
    "'''\n",
    "\n",
    "def build_decoder(latent_dim, output_dim, rest_range, observed_range, observed_resolution, upsample_factor):\n",
    "    # Inputs: latent vector and scalar z\n",
    "    latent_input = Input(shape=(latent_dim,), name='latent_input')\n",
    "    z_input = Input(shape=(1,), name='z')  # Scalar input for z\n",
    "    \n",
    "    # Step 1: Fully Connected Layers to generate rest frame representation\n",
    "    x = Dense(64)(latent_input)\n",
    "    x = PReLU()(x)    \n",
    "    x = Dense(256)(x)\n",
    "    x = PReLU()(x)\n",
    "    x = Dense(1024)(x)\n",
    "    x = PReLU()(x)\n",
    "\n",
    "    # Generate rest frame grid\n",
    "    min_rest_x, max_rest_x = rest_range\n",
    "    rest_length = int((max_rest_x - min_rest_x) / observed_resolution * upsample_factor)\n",
    "    rest_x = tf.linspace(min_rest_x, max_rest_x, rest_length)\n",
    "\n",
    "    # Upsample Layer \n",
    "    x = Dense(rest_length)(x)\n",
    "    x = PReLU()(x)\n",
    "\n",
    "    # Define a Lambda layer to handle TensorFlow operations on Keras tensors\n",
    "    def slice_and_downsample(inputs):\n",
    "        x, z_input = inputs\n",
    "\n",
    "        # Compute Boundary Indexes based on rest_x, observed_range, and z_input\n",
    "        z_input = tf.squeeze(z_input, axis=-1)  # Remove the extra dimension from z_input if needed\n",
    "        min_rest_obs = observed_range[0] / (1 + z_input)\n",
    "        max_rest_obs = observed_range[1] / (1 + z_input)\n",
    "\n",
    "        # Broadcast rest_x to match the batch size (the first dimension of x)\n",
    "        rest_x_broadcasted = tf.broadcast_to(rest_x, [tf.shape(x)[0], rest_length])\n",
    "\n",
    "        # Find start and stop indices using TensorFlow operations\n",
    "        start_indices = tf.argmin(tf.abs(rest_x_broadcasted - tf.expand_dims(min_rest_obs, axis=-1)), axis=-1)\n",
    "        stop_indices = tf.argmin(tf.abs(rest_x_broadcasted - tf.expand_dims(max_rest_obs, axis=-1)), axis=-1)\n",
    "\n",
    "        # Use TensorFlow's batch-wise slicing mechanism to slice based on indices\n",
    "        batch_size = tf.shape(x)[0]\n",
    "\n",
    "        # Generate index grid for each batch\n",
    "        indices = tf.range(batch_size)\n",
    "\n",
    "        # Gather the slice from each batch using start and stop indices\n",
    "        sliced_x = tf.map_fn(\n",
    "            lambda idx: x[idx, start_indices[idx]:stop_indices[idx]:upsample_factor], \n",
    "            indices, dtype=tf.float32\n",
    "        )\n",
    "\n",
    "        return sliced_x\n",
    "\n",
    "    # Define the output shape manually\n",
    "    def compute_output_shape(input_shapes):\n",
    "        latent_shape, z_shape = input_shapes\n",
    "        return (latent_shape[0], output_dim)\n",
    "\n",
    "    # Use Lambda layer to apply the slicing and downsampling\n",
    "    output = Lambda(slice_and_downsample, output_shape=compute_output_shape)([x, z_input])\n",
    "\n",
    "    # Reshape the final output to the desired dimensions\n",
    "    output = Reshape((output_dim, 1))(output)\n",
    "\n",
    "    return Model([latent_input, z_input], output, name='decoder')\n",
    "def build_autoencoder(input_shape, latent_dim, z_range, observed_range, observed_resolution, upsample_factor):\n",
    "    spectra_input = Input(shape=input_shape, name='spectra_input')\n",
    "    \n",
    "    # Building the encoder (you would define build_encoder separately)\n",
    "    encoder = build_encoder(input_shape, latent_dim)\n",
    "    latent_space = encoder(spectra_input)\n",
    "    \n",
    "    # Scalar z input for each iteration\n",
    "    z_input = Input(shape=(1,), name='z_input')\n",
    "    \n",
    "    # Building the decoder, which now takes both latent vector and scalar z\n",
    "\n",
    "    rest_range = [observed_range[0]/(1+z_range[1]), observed_range[1]/(1+z_range[0])]\n",
    "\n",
    "    decoder = build_decoder(latent_dim, input_shape[0], rest_range, observed_range, observed_resolution, upsample_factor)\n",
    "    reconstructed_output = decoder([latent_space, z_input])\n",
    "\n",
    "    # Define the complete autoencoder model\n",
    "    return Model(inputs=[spectra_input, z_input], outputs=reconstructed_output, name='autoencoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"autoencoder\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"autoencoder\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ spectra_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">26,522,634</span> │ spectra_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ z_input             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,652,592</span> │ encoder[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │ z_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ spectra_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1500\u001b[0m, \u001b[38;5;34m1\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │ \u001b[38;5;34m26,522,634\u001b[0m │ spectra_input[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ z_input             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1500\u001b[0m, \u001b[38;5;34m1\u001b[0m)   │  \u001b[38;5;34m1,652,592\u001b[0m │ encoder[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │ z_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">28,175,226</span> (107.48 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m28,175,226\u001b[0m (107.48 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">28,175,226</span> (107.48 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m28,175,226\u001b[0m (107.48 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "observed_range = [3550, 10400]\n",
    "observed_length = 1500\n",
    "observed_resolution = (observed_range[1]-observed_range[0])/observed_length\n",
    "z_range = [1.5, 2.2]\n",
    "\n",
    "autoencoder = build_autoencoder(input_shape=(1500, 1), latent_dim=10, z_range=z_range, observed_range=observed_range, observed_resolution=observed_resolution, upsample_factor=2)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 392\n",
      "Test set size: 99\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from astropy.io import fits\n",
    "\n",
    "data_dir = '/Users/tkiker/Documents/GitHub/AGN-UMAP/data/sdss_spectra'\n",
    "\n",
    "file_names = []\n",
    "spectra = []\n",
    "zs = []\n",
    "\n",
    "x = np.linspace(observed_range[0], observed_range[1], observed_length)\n",
    "\n",
    "for file_name in os.listdir(data_dir)[0:500]:\n",
    "    hdul = fits.open(os.path.join(data_dir, file_name))\n",
    "    \n",
    "    z = hdul[2].data['z'][0]\n",
    "\n",
    "    if z_range[0] <= z <= z_range[1]:\n",
    "\n",
    "        data = hdul[1].data\n",
    "\n",
    "        wavelength = 10**data[\"loglam\"]\n",
    "        flux = data[\"flux\"]\n",
    "\n",
    "        # Apply a Gaussian filter to smooth the flux\n",
    "        flux = gaussian_filter1d(flux, sigma=3)\n",
    "        \n",
    "        # Interpolate the flux to match the x array\n",
    "        flux = np.interp(x, wavelength, flux)\n",
    "\n",
    "        # Convert to rest-frame wavelength\n",
    "        rest_wavelength = x / (1 + z)\n",
    "\n",
    "        # Normalize the flux using a range of rest wavelengths\n",
    "        norm_mask = np.logical_and(rest_wavelength >= 2000, rest_wavelength <= 2500)\n",
    "        flux /= np.median(flux[norm_mask])\n",
    "\n",
    "        spectra.append(flux)\n",
    "        file_names.append(file_name)\n",
    "        zs.append(z)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "spectra = np.array(spectra)\n",
    "zs = np.array(zs)\n",
    "file_names = np.array(file_names)\n",
    "\n",
    "# Split the data into training and test sets (you can adjust test_size and random_state)\n",
    "spectra_train, spectra_test, zs_train, zs_test, file_names_train, file_names_test = train_test_split(\n",
    "    spectra, zs, file_names, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training set size:\", len(spectra_train))\n",
    "print(\"Test set size:\", len(spectra_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:From /opt/homebrew/Caskroom/miniconda/base/envs/agn310b/lib/python3.10/site-packages/tensorflow/python/util/deprecation.py:660: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling Lambda.call().\n\n\u001b[1mTensors in list passed to 'values' of 'Pack' Op have types [int32, int64] that don't all match.\u001b[0m\n\nArguments received by Lambda.call():\n  • inputs=['tf.Tensor(shape=(None, 1336), dtype=float32)', 'tf.Tensor(shape=(None, 1), dtype=float32)']\n  • mask=['None', 'None']\n  • training=True",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model using the `fit` method\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mspectra_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzs_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Training data for both inputs\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspectra_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# The target output is the same as the input in autoencoders\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                  \u001b[49m\u001b[38;5;66;43;03m# Number of epochs to train\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# Size of each batch\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mspectra_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzs_test\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspectra_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Validation data\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m                   \u001b[49m\u001b[38;5;66;43;03m# Show training progress\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# After training, you can evaluate the model on the test set or save the model\u001b[39;00m\n\u001b[1;32m     12\u001b[0m autoencoder\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoencoder_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/agn310b/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[19], line 148\u001b[0m, in \u001b[0;36mbuild_decoder.<locals>.slice_and_downsample\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m indices \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrange(batch_size)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# Gather the slice from each batch using start and stop indices\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m sliced_x \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\u001b[43mstop_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\u001b[43mupsample_factor\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sliced_x\n",
      "Cell \u001b[0;32mIn[19], line 149\u001b[0m, in \u001b[0;36mbuild_decoder.<locals>.slice_and_downsample.<locals>.<lambda>\u001b[0;34m(idx)\u001b[0m\n\u001b[1;32m    145\u001b[0m indices \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrange(batch_size)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# Gather the slice from each batch using start and stop indices\u001b[39;00m\n\u001b[1;32m    148\u001b[0m sliced_x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmap_fn(\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m idx: \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\u001b[43mstop_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\u001b[43mupsample_factor\u001b[49m\u001b[43m]\u001b[49m, \n\u001b[1;32m    150\u001b[0m     indices, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[1;32m    151\u001b[0m )\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sliced_x\n",
      "\u001b[0;31mTypeError\u001b[0m: Exception encountered when calling Lambda.call().\n\n\u001b[1mTensors in list passed to 'values' of 'Pack' Op have types [int32, int64] that don't all match.\u001b[0m\n\nArguments received by Lambda.call():\n  • inputs=['tf.Tensor(shape=(None, 1336), dtype=float32)', 'tf.Tensor(shape=(None, 1), dtype=float32)']\n  • mask=['None', 'None']\n  • training=True"
     ]
    }
   ],
   "source": [
    "# Train the model using the `fit` method\n",
    "history = autoencoder.fit(\n",
    "    [spectra_train, zs_train],  # Training data for both inputs\n",
    "    spectra_train,              # The target output is the same as the input in autoencoders\n",
    "    epochs=50,                  # Number of epochs to train\n",
    "    batch_size=32,              # Size of each batch\n",
    "    validation_data=([spectra_test, zs_test], spectra_test),  # Validation data\n",
    "    verbose=1                   # Show training progress\n",
    ")\n",
    "\n",
    "# After training, you can evaluate the model on the test set or save the model\n",
    "autoencoder.save('autoencoder_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training history\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_dict['loss'])\n",
    "plt.plot(history_dict['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agn310b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
